{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully with shape: (476, 133)\n",
      "   study_no   group  group_1  Abiotrophia  Acetatifactor  \\\n",
      "0  H1700097  Cancer        1          0.0            0.0   \n",
      "1  H1700105  Cancer        1          0.0            0.0   \n",
      "2  H1700084  Cancer        1          0.0            0.0   \n",
      "3  H1700109  Cancer        1          0.0            0.0   \n",
      "4  H1700098  Cancer        1          0.0            0.0   \n",
      "\n",
      "   Acholeplasmataceae_uc  Actinobacillus  Actinobaculum  Actinomyces  \\\n",
      "0                    0.0             0.0            0.0          3.7   \n",
      "1                    0.0             0.0            0.0          0.7   \n",
      "2                    0.0             0.0            0.0          1.3   \n",
      "3                    0.0             0.0            0.1          0.8   \n",
      "4                    0.0             0.0            0.0          2.5   \n",
      "\n",
      "   Aggregatibacter  ...  Xanthomonas  Cloacibacterium  Legionella  Variovorax  \\\n",
      "0              0.0  ...          0.0              0.0         0.0         0.0   \n",
      "1              0.9  ...          0.0              0.0         0.0         0.0   \n",
      "2              0.0  ...          0.0              0.0         0.0         0.0   \n",
      "3              0.0  ...          0.0              0.2         0.0         0.0   \n",
      "4              0.0  ...          0.0              0.0         0.0         0.0   \n",
      "\n",
      "   Bergeriella  Ethanoligenens  Lentimicrobiaceae_uc  Moraxella  \\\n",
      "0          0.0             0.0                   0.0        0.0   \n",
      "1          0.0             0.0                   0.0        0.0   \n",
      "2          0.0             0.0                   0.0        0.0   \n",
      "3          0.0             0.0                   0.0        0.0   \n",
      "4          0.0             0.0                   0.0        0.0   \n",
      "\n",
      "   Mycoplasma_g13  Streptobacillus  \n",
      "0             0.0              0.0  \n",
      "1             0.0              0.0  \n",
      "2             0.0              0.0  \n",
      "3             0.0              0.0  \n",
      "4             0.0              0.0  \n",
      "\n",
      "[5 rows x 133 columns]\n",
      "Combined data saved to /home/user/Desktop/PC_Analysis/Train_Result_RFE/Model_Run_1730946141/used_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import glob\n",
    "import time\n",
    "import re  # To help extract version numbers from filenames\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a timestamp for this specific model run\n",
    "timestamp = int(time.time())\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = os.path.join(current_dir, 'Train_Result_RFE')\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create a subdirectory for the current model run\n",
    "run_output_dir = os.path.join(output_dir, f'Model_Run_{timestamp}')\n",
    "os.makedirs(run_output_dir, exist_ok=True)\n",
    "\n",
    "# Find CSV or Excel file in the current directory\n",
    "data_file = None\n",
    "for ext in ['csv', 'xlsx', 'xls']:\n",
    "    files = glob.glob(os.path.join(current_dir, f'*.{ext}'))\n",
    "    if files:\n",
    "        data_file = files[0]  # Take the first file found\n",
    "        break\n",
    "\n",
    "# Raise an error if no data file is found\n",
    "if not data_file:\n",
    "    raise FileNotFoundError(\"No CSV or Excel file found in the current directory.\")\n",
    "\n",
    "# Read the data file\n",
    "if data_file.endswith('.csv'):\n",
    "    data = pd.read_csv(data_file)\n",
    "elif data_file.endswith(('.xlsx', '.xls')):\n",
    "    data = pd.read_excel(data_file)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported file format. Only CSV or Excel files are supported.\")\n",
    "\n",
    "# Display basic information about the data\n",
    "print(f\"Data loaded successfully with shape: {data.shape}\")\n",
    "print(data.head())\n",
    "\n",
    "# Handle missing values\n",
    "if data.isnull().values.any():\n",
    "    print(\"Warning: There are missing values in the dataset. Dropping missing values.\")\n",
    "    data = data.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Split the entire DataFrame\n",
    "cur_test_size = 0.3\n",
    "random_state = 42  # Adjust as needed\n",
    "\n",
    "# Stratify by 'group_1' to maintain label proportions\n",
    "train_data, val_data = train_test_split(\n",
    "    data, test_size=cur_test_size, stratify=data['group_1'], random_state=random_state\n",
    ")\n",
    "\n",
    "# Add 'Set' column to indicate the dataset\n",
    "train_data = train_data.copy()\n",
    "train_data['Set'] = 'Discovery'\n",
    "\n",
    "val_data = val_data.copy()\n",
    "val_data['Set'] = 'Validation'\n",
    "\n",
    "# Rearrange columns so 'Set' and 'label' are first\n",
    "cols = ['Set'] + [col for col in train_data.columns if col not in ['Set']]\n",
    "train_data = train_data[cols]\n",
    "val_data = val_data[cols]\n",
    "# Move 'Set' column to the front\n",
    "cols = ['Set'] + [col for col in train_data.columns if col != 'Set']\n",
    "train_data = train_data[cols]\n",
    "val_data = val_data[cols]\n",
    "\n",
    "used_data =pd.concat([train_data,val_data])\n",
    "used_data = used_data.sort_values(by='study_no')\n",
    "# Define the file path\n",
    "combined_file_path = os.path.join(run_output_dir, 'used_data.xlsx')\n",
    "\n",
    "# Save to Excel\n",
    "used_data.to_excel(combined_file_path, index=True)\n",
    "\n",
    "print(f\"Combined data saved to {combined_file_path}\")\n",
    "\n",
    "feature_columns = data.columns[3:]  # Adjust as needed\n",
    "label_column = 'group_1'\n",
    "\n",
    "# Extract features and labels for training set\n",
    "X_train = train_data[feature_columns].astype(float)\n",
    "y_train = train_data[label_column].astype(float)\n",
    "\n",
    "# Extract features and labels for validation set\n",
    "X_val = val_data[feature_columns].astype(float)\n",
    "y_val = val_data[label_column].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Set</th>\n",
       "      <th>study_no</th>\n",
       "      <th>group</th>\n",
       "      <th>group_1</th>\n",
       "      <th>Abiotrophia</th>\n",
       "      <th>Acetatifactor</th>\n",
       "      <th>Acholeplasmataceae_uc</th>\n",
       "      <th>Actinobacillus</th>\n",
       "      <th>Actinobaculum</th>\n",
       "      <th>Actinomyces</th>\n",
       "      <th>...</th>\n",
       "      <th>Xanthomonas</th>\n",
       "      <th>Cloacibacterium</th>\n",
       "      <th>Legionella</th>\n",
       "      <th>Variovorax</th>\n",
       "      <th>Bergeriella</th>\n",
       "      <th>Ethanoligenens</th>\n",
       "      <th>Lentimicrobiaceae_uc</th>\n",
       "      <th>Moraxella</th>\n",
       "      <th>Mycoplasma_g13</th>\n",
       "      <th>Streptobacillus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>H1900608</td>\n",
       "      <td>Control</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>H1700032</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>H1900723</td>\n",
       "      <td>Control</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>H1900166</td>\n",
       "      <td>Control</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>H1900078</td>\n",
       "      <td>Control</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>H1900354</td>\n",
       "      <td>Control</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>H1700031</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.86</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>H1900400</td>\n",
       "      <td>Control</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.78</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>H1900747</td>\n",
       "      <td>Control</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>H1700093</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows × 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Set  study_no    group  group_1  Abiotrophia  Acetatifactor  \\\n",
       "350  Discovery  H1900608  Control        0         0.00            0.0   \n",
       "64   Discovery  H1700032   Cancer        1         0.01            0.0   \n",
       "412  Discovery  H1900723  Control        0         0.00            0.0   \n",
       "166  Discovery  H1900166  Control        0         0.01            0.0   \n",
       "128  Discovery  H1900078  Control        0         0.00            0.0   \n",
       "..         ...       ...      ...      ...          ...            ...   \n",
       "251  Discovery  H1900354  Control        0         0.00            0.0   \n",
       "63   Discovery  H1700031   Cancer        1         0.00            0.0   \n",
       "267  Discovery  H1900400  Control        0         0.00            0.0   \n",
       "424  Discovery  H1900747  Control        0         0.00            0.0   \n",
       "22   Discovery  H1700093   Cancer        1         0.00            0.0   \n",
       "\n",
       "     Acholeplasmataceae_uc  Actinobacillus  Actinobaculum  Actinomyces  ...  \\\n",
       "350                    0.0            0.00           0.02         1.30  ...   \n",
       "64                     0.0            0.00           0.08         3.70  ...   \n",
       "412                    0.0            0.11           0.00         1.01  ...   \n",
       "166                    0.0            0.00           0.00         0.82  ...   \n",
       "128                    0.0            0.00           0.02         1.39  ...   \n",
       "..                     ...             ...            ...          ...  ...   \n",
       "251                    0.0            0.00           0.00         2.56  ...   \n",
       "63                     0.0            0.00           0.00         3.86  ...   \n",
       "267                    0.0            0.00           0.02         1.78  ...   \n",
       "424                    0.0            0.00           0.33         2.49  ...   \n",
       "22                     0.0            0.00           0.00         0.30  ...   \n",
       "\n",
       "     Xanthomonas  Cloacibacterium  Legionella  Variovorax  Bergeriella  \\\n",
       "350          0.0              0.0         0.0         0.0         0.00   \n",
       "64           0.0              0.0         0.0         0.0         0.00   \n",
       "412          0.0              0.0         0.0         0.0         0.00   \n",
       "166          0.0              0.0         0.0         0.0         0.00   \n",
       "128          0.0              0.0         0.0         0.0         0.03   \n",
       "..           ...              ...         ...         ...          ...   \n",
       "251          0.0              0.0         0.0         0.0         0.00   \n",
       "63           0.0              0.0         0.0         0.0         0.00   \n",
       "267          0.0              0.0         0.0         0.0         0.00   \n",
       "424          0.0              0.0         0.0         0.0         0.00   \n",
       "22           0.0              0.0         0.2         0.0         0.00   \n",
       "\n",
       "     Ethanoligenens  Lentimicrobiaceae_uc  Moraxella  Mycoplasma_g13  \\\n",
       "350            0.00                   0.0        0.0             0.0   \n",
       "64             0.00                   0.0        0.0             0.0   \n",
       "412            0.00                   0.0        0.0             0.0   \n",
       "166            0.00                   0.0        0.0             0.0   \n",
       "128            0.00                   0.0        0.0             0.0   \n",
       "..              ...                   ...        ...             ...   \n",
       "251            0.00                   0.0        0.0             0.0   \n",
       "63             0.00                   0.0        0.0             0.0   \n",
       "267            0.00                   0.0        0.0             0.0   \n",
       "424            0.01                   0.0        0.0             0.0   \n",
       "22             0.00                   0.0        0.0             0.0   \n",
       "\n",
       "     Streptobacillus  \n",
       "350              0.0  \n",
       "64               0.0  \n",
       "412              0.0  \n",
       "166              0.0  \n",
       "128              0.0  \n",
       "..               ...  \n",
       "251              0.0  \n",
       "63               0.0  \n",
       "267              0.0  \n",
       "424              0.0  \n",
       "22               0.0  \n",
       "\n",
       "[333 rows x 134 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Abiotrophia', 'Acetatifactor', 'Acholeplasmataceae_uc',\n",
       "       'Actinobacillus', 'Actinobaculum', 'Actinomyces', 'Aggregatibacter',\n",
       "       'Akkermansia', 'Alistipes', 'Alloprevotella',\n",
       "       ...\n",
       "       'Xanthomonas', 'Cloacibacterium', 'Legionella', 'Variovorax',\n",
       "       'Bergeriella', 'Ethanoligenens', 'Lentimicrobiaceae_uc', 'Moraxella',\n",
       "       'Mycoplasma_g13', 'Streptobacillus'],\n",
       "      dtype='object', length=130)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF data saved to /home/user/Desktop/PC_Analysis/Train_Result_RFE/possible_multicollinearity_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "features = data[feature_columns]  # replace 'data' with your actual DataFrame if necessary\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = features.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]\n",
    "\n",
    "vif_data_mul = vif_data[vif_data['VIF']>10]\n",
    "output_file = os.path.join(output_dir, \"possible_multicollinearity_features.xlsx\")\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "vif_data_mul.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"VIF data saved to {output_file}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the Discovery, Validation Set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 선택 및 모델 deploying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326c81b300d644b68389ad1dcb63614e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Choose Model:', options=('xgb', 'rf', 'catboost', 'gbm', 'lgbm'), value='xgb')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e09a0cede8241e1bd30b72349331dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Model', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import shap\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 사용성 증가를 위해 widget추가\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Function to calculate F-Score\n",
    "def f_score(y_test, preds, beta=1):\n",
    "    tp = np.sum((preds == 1) & (y_test == 1))\n",
    "    fp = np.sum((preds == 1) & (y_test == 0))\n",
    "    fn = np.sum((preds == 0) & (y_test == 1))\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    fscore = (1 + beta ** 2) * (precision * recall) / ((beta ** 2 * precision) + recall + 1e-8)\n",
    "\n",
    "    return fscore\n",
    "\n",
    "# Argument parser definition for model parameters\n",
    "parser = argparse.ArgumentParser(description='Train and evaluate a binary classification model.')\n",
    "parser.add_argument('--model', type=str, default='xgb', choices=['xgb', 'rf', 'catboost', 'gbm', 'lgbm'],\n",
    "                    help='Choose a model: xgb (XGBoost), rf (Random Forest), catboost, gbm, lgbm (LightGBM).')\n",
    "parser.add_argument('--random_state', type=int, default=42, help='Random state for reproducibility.')\n",
    "parser.add_argument('--n_estimators', type=int, default=100, help='Number of trees/estimators for the model.')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.1, help='Learning rate for boosting models.')\n",
    "parser.add_argument('--max_depth', type=int, default=6, help='Maximum depth for tree-based models.')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# Function to choose and run the model\n",
    "def run_model(model_choice):\n",
    "    # Create output directory for current model run\n",
    "    run_output_dir = os.path.join('Train_Result_RFE', f'Model_Run_{model_choice}_{int(time.time())}')\n",
    "    os.makedirs(run_output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize model based on the user's choice\n",
    "    if model_choice == 'xgb':\n",
    "        model = xgb.XGBClassifier(n_estimators=args.n_estimators, learning_rate=args.learning_rate, max_depth=args.max_depth, random_state=args.random_state,tree_method='gpu_hist')\n",
    "    elif model_choice == 'rf':\n",
    "        model = RandomForestClassifier(n_estimators=args.n_estimators, max_depth=args.max_depth, random_state=args.random_state)\n",
    "    elif model_choice == 'catboost':\n",
    "        model = CatBoostClassifier(iterations=args.n_estimators, depth=args.max_depth, learning_rate=args.learning_rate, random_state=args.random_state, verbose=0)\n",
    "    elif model_choice == 'gbm':\n",
    "        model = GradientBoostingClassifier(n_estimators=args.n_estimators, learning_rate=args.learning_rate, max_depth=args.max_depth, random_state=args.random_state)\n",
    "    elif model_choice == 'lgbm':\n",
    "        model = lgb.LGBMClassifier(n_estimators=args.n_estimators, learning_rate=args.learning_rate, max_depth=args.max_depth, random_state=args.random_state)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_choice}\")\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions and evaluations\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f_score(y_val, y_pred)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Model: {model_choice}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "    # Save results\n",
    "    result_path = os.path.join(run_output_dir, f'Results_{model_choice}.txt')\n",
    "    with open(result_path, 'w') as f:\n",
    "        f.write(f\"Model: {model_choice}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "        f.write(f\"F1 Score: {f1}\\n\")\n",
    "\n",
    "    print(f\"Results saved to {result_path}\")\n",
    "\n",
    "    # Ensure 'study_no' column exists in data and assign Discovery/Validation\n",
    "    train_indices = X_train.index\n",
    "    val_indices = X_val.index\n",
    "\n",
    "    if 'study_no' in data.columns:\n",
    "        # Assign 'Discovery' to rows in data where study_no is in X_train\n",
    "        data.loc[data.index.isin(train_indices), 'group_2'] = 'Discovery'\n",
    "\n",
    "        # Assign 'Validation' to rows in data where study_no is in X_val\n",
    "        data.loc[data.index.isin(val_indices), 'group_2'] = 'Validation'\n",
    "\n",
    "        # Save the modified data to an Excel file\n",
    "        modified_data_path = os.path.join(run_output_dir, 'Modified_Data_File.xlsx')\n",
    "        data.to_excel(modified_data_path, index=False)\n",
    "        print(f\"Modified data file saved to {modified_data_path}\")\n",
    "    else:\n",
    "        print(\"'study_no' column not found in data.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Model selection widget and execution button\n",
    "model_selector = widgets.Dropdown(\n",
    "    options=['xgb', 'rf', 'catboost', 'gbm', 'lgbm'],\n",
    "    value='xgb',\n",
    "    description='Choose Model:',\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(description=\"Run Model\")\n",
    "\n",
    "def on_button_click(b):\n",
    "    run_model(model_selector.value)\n",
    "\n",
    "run_button.on_click(on_button_click)\n",
    "\n",
    "# Display widgets\n",
    "display(model_selector)\n",
    "display(run_button)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFE feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcc9eec261c4a65b3e5b27cdb474559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Choose Model:', options=('xgb', 'rf', 'catboost', 'gbm', 'lgbm'), value='xgb')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a139be13eaf64f44accf911862a0ee5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Model', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b152741c12164b289510aecaef013ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "import optuna\n",
    "\n",
    "# FNR evaluation function\n",
    "def FNR_eval(y_pred_proba, y_true):\n",
    "    # Convert predicted probabilities to binary labels (threshold of 0.5)\n",
    "    y_pred_label = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_label).ravel()\n",
    "    \n",
    "    # Calculate FNR (False Negative Rate)\n",
    "    fnr = fn / (fn + tp)  # Correct formula for FNR is FN / (FN + TP)\n",
    "    \n",
    "    return fnr\n",
    "\n",
    "\n",
    "\n",
    "# Create output directory for current model run\n",
    "run_output_dir = os.path.join('Train_Result_RFE', f'Model_Run_{int(time.time())}')\n",
    "os.makedirs(run_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Set up output widget to display results\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "\n",
    "# Function to choose and run the model\n",
    "def run_model(model_choice):\n",
    "    # Initialize model based on the user's choice\n",
    "    run_output_dir = os.path.join('Train_Result_RFE', f'Model_Run_{model_choice}_{int(time.time())}')\n",
    "    os.makedirs(run_output_dir, exist_ok=True)\n",
    "    # Initialize model based on the user's choice\n",
    "    if model_choice == 'xgb':\n",
    "        model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, tree_method = 'hist',device = \"cuda\")\n",
    "    elif model_choice == 'rf':\n",
    "        model = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42)\n",
    "    elif model_choice == 'catboost':\n",
    "        model = CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, random_state=42, verbose=0)\n",
    "    elif model_choice == 'gbm':\n",
    "        model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "    elif model_choice == 'lgbm':\n",
    "        model = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_choice}\")\n",
    "\n",
    "    # Apply RFE to the model to select top 10 features\n",
    "    rfe = RFE(model, n_features_to_select=20)\n",
    "    rfe.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions and evaluations\n",
    "    train_pred = rfe.predict(X_train)\n",
    "    val_pred = rfe.predict(X_val)\n",
    "    y_pred_proba = rfe.predict_proba(X_val)[:, 1]  # Get predicted probabilities for class 1\n",
    "    accuracy = accuracy_score(y_val, val_pred)\n",
    "    fnr = FNR_eval(y_pred_proba, y_val)\n",
    "    print(f\"Model: {model_choice}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Define the result directory and result file paths\n",
    "    result_file = os.path.join(run_output_dir, f'Results_{model_choice}.txt')\n",
    "\n",
    "    # Save results to text file\n",
    "    with open(result_file, 'w') as f:\n",
    "        f.write(f\"Model: {model_choice}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "        f.write(f\"FNR: {fnr}\\n\")\n",
    "\n",
    "    # 혼동 행렬 계산\n",
    "    conf_matrix = confusion_matrix(y_val, val_pred)\n",
    "\n",
    "    # 혼동 행렬 시각화\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Control', 'Cancer'], yticklabels=['Control', 'Cancer'], annot_kws={\"size\": 14})\n",
    "    plt.title('Confusion Matrix on Validation Set ( Split)', fontsize=15)\n",
    "    plt.xlabel('Predicted Label', fontsize=13)\n",
    "    plt.ylabel('True Label', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(run_output_dir, f'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # Save feature importance and evaluation\n",
    "    important_features = X_train.columns[rfe.support_]\n",
    "    print(f\"Top Features Selected by RFE: {important_features}\")\n",
    "\n",
    "    # Save feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': important_features,\n",
    "        'Importance': rfe.estimator_.feature_importances_\n",
    "    })\n",
    "    top_featuers = importance_df.sort_values(by='Importance', ascending=False).head(20)\n",
    "    excel_file = os.path.join(run_output_dir, f'feature_importances_{model_choice}.xlsx')\n",
    "    importance_df.to_excel(excel_file, index=False)\n",
    "    # Plot feature importance\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(top_featuers)))\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=top_featuers, palette=colors)\n",
    "    plt.title('Top 20 Important Features Selected by RFE')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(run_output_dir, 'feature_importance.png'))  # Save to result_dir\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # SHAP explanation (on the selected features)\n",
    "    explainer = shap.TreeExplainer(rfe.estimator_)\n",
    "    shap_values = explainer.shap_values(X_val[important_features])\n",
    "    if model_choice == 'rf':\n",
    "        # For RandomForest, shap_values has shape (n_samples, n_features, n_classes)\n",
    "        # We need to select shap_values for the positive class (e.g., class 1 for binary classification)\n",
    "        # SHAP summary plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values[:,:,1], X_val[important_features], plot_type='dot', show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(run_output_dir, 'shap_feature_importance.png'))  # Save to result_dir\n",
    "        plt.close()\n",
    "    else:\n",
    "        # For other models (like XGB, LGBM, etc.), we can use shap_values directly\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, X_val[important_features], show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(run_output_dir, 'shap_feature_importance.png'))  # Save to result_dir\n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "    # ROC curve and AUC score\n",
    "    y_pred_proba = rfe.predict_proba(X_val)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "    auc_score = roc_auc_score(y_val, y_pred_proba)\n",
    "\n",
    "    # ROC curve plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {auc_score:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16)\n",
    "    plt.legend(loc='lower right', fontsize=12)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(run_output_dir, 'roc_curve.png'))  # Save to result_dir\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"All results are saved in {run_output_dir}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model_selector = widgets.Dropdown(\n",
    "    options=['xgb', 'rf', 'catboost', 'gbm', 'lgbm'],\n",
    "    value='xgb',\n",
    "    description='Choose Model:',\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(description=\"Run Model\")\n",
    "# Label to indicate model status\n",
    "status_label = widgets.Label(value='')\n",
    "\n",
    "\n",
    "def on_button_click(b):\n",
    "    # Update status to \"Running...\"\n",
    "    status_label.value = \"Running...\"\n",
    "    run_model(model_selector.value)\n",
    "    # After running, update status to \"Done!\"\n",
    "    status_label.value = \"Done!\"\n",
    "\n",
    "run_button.on_click(on_button_click)\n",
    "\n",
    "# Display widgets\n",
    "display(model_selector)\n",
    "display(run_button)\n",
    "display(status_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE with bootstraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall xgboost -y\n",
    "# !pip install xgboost --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cdf06633ca415ba57c280d36d1674f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Choose Model:', options=('xgb', 'rf', 'catboost', 'gbm', 'lgbm'), value='xgb')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6494e5996b437e84f7dfebdac38fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=100, description='Bootstraps:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9d895b3bbd43fd9df6bbabd3a96b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Model', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a83207dc16247b9b6827a3fa02fcb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import shap\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "# 사용성 증가를 위해 widget추가\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.utils import resample  # Import resample for bootstrap\n",
    "from sklearn.feature_selection import RFE\n",
    "# Aggregate selected features\n",
    "from collections import Counter\n",
    "from sklearn.base import clone\n",
    "\n",
    "def run_model(model_choice, n_bootstraps=100):\n",
    "\n",
    "    # Initialize model based on the user's choice\n",
    "    if model_choice == 'xgb':\n",
    "        model_init = xgb.XGBClassifier(\n",
    "            n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, tree_method='hist',device='cuda'\n",
    "        )\n",
    "    elif model_choice == 'rf':\n",
    "        model_init = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42)\n",
    "    elif model_choice == 'catboost':\n",
    "        model_init = CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, random_state=42, verbose=0)\n",
    "    elif model_choice == 'gbm':\n",
    "        model_init = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "    elif model_choice == 'lgbm':\n",
    "        model_init = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_choice}\")\n",
    "\n",
    "    # Initialize lists to store performance metrics\n",
    "    accuracies = []\n",
    "    auc_scores = []\n",
    "    selected_features_list = []\n",
    "    feature_importances_dict = {}\n",
    "\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        # Resample the training data with replacement\n",
    "        X_train_bootstrap, y_train_bootstrap = resample(X_train, y_train, replace=True)\n",
    "\n",
    "        # Initialize a new model instance for each bootstrap\n",
    "        model = clone(model_init)\n",
    "\n",
    "        # Apply RFE on the bootstrapped data\n",
    "        rfe = RFE(model, n_features_to_select=20)\n",
    "        rfe.fit(X_train_bootstrap, y_train_bootstrap)\n",
    "\n",
    "        # Get the selected features for this bootstrap\n",
    "        selected_features = X_train.columns[rfe.support_]\n",
    "        selected_features_list.append(selected_features)\n",
    "\n",
    "        # Reduce the bootstrapped data to only the selected features\n",
    "        X_train_bootstrap_selected = X_train_bootstrap[selected_features]\n",
    "        X_val_selected = X_val[selected_features]  # Adjust validation set accordingly\n",
    "\n",
    "        # Fit the model on the bootstrap sample with selected features\n",
    "        model.fit(X_train_bootstrap_selected, y_train_bootstrap)\n",
    "\n",
    "        # Predictions and evaluations on validation set\n",
    "        y_pred = model.predict(X_val_selected)\n",
    "        accuracies.append(accuracy_score(y_val, y_pred))\n",
    "\n",
    "        y_pred_proba = model.predict_proba(X_val_selected)[:, 1]\n",
    "        auc_scores.append(roc_auc_score(y_val, y_pred_proba))\n",
    "\n",
    "        # Collect feature importances\n",
    "        for feature, importance in zip(selected_features, model.feature_importances_):\n",
    "            if feature in feature_importances_dict:\n",
    "                feature_importances_dict[feature].append(importance)\n",
    "            else:\n",
    "                feature_importances_dict[feature] = [importance]\n",
    "\n",
    "    # Aggregate selected features\n",
    "    feature_frequency = Counter([feat for sublist in selected_features_list for feat in sublist])\n",
    "    # Determine features selected most frequently\n",
    "    most_common_features = feature_frequency.most_common()\n",
    "    # Convert the list of tuples into a DataFrame\n",
    "    feature_counts_df = pd.DataFrame(most_common_features, columns=['Feature', 'Count'])\n",
    "    # Sort by 'Count' in descending order\n",
    "    feature_counts_df = feature_counts_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Save feature counts to Excel\n",
    "    result_dir = 'results'  # Define the result directory\n",
    "    os.makedirs(result_dir, exist_ok=True)  # Ensure the directory exists\n",
    "    output_excel_file = os.path.join(result_dir, 'feature_selection_counts.xlsx')\n",
    "    feature_counts_df.to_excel(output_excel_file, index=False)\n",
    "    print(f\"Feature selection counts saved to {output_excel_file}\")\n",
    "\n",
    "    # Calculate mean and confidence intervals\n",
    "    accuracy_mean = np.mean(accuracies)\n",
    "    accuracy_ci = np.percentile(accuracies, [2.5, 97.5])\n",
    "\n",
    "    auc_mean = np.mean(auc_scores)\n",
    "    auc_ci = np.percentile(auc_scores, [2.5, 97.5])\n",
    "\n",
    "    # Calculate mean feature importances\n",
    "    mean_feature_importances = {feature: np.mean(importances) for feature, importances in feature_importances_dict.items()}\n",
    "    importance_df = pd.DataFrame(list(mean_feature_importances.items()), columns=['Feature', 'Importance'])\n",
    "    # Sort by 'Importance' in descending order\n",
    "    importance_df_sorted = importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Model: {model_choice}\")\n",
    "    print(f\"Accuracy: {accuracy_mean:.4f} (95% CI: {accuracy_ci[0]:.4f}, {accuracy_ci[1]:.4f})\")\n",
    "    print(f\"AUC: {auc_mean:.4f} (95% CI: {auc_ci[0]:.4f}, {auc_ci[1]:.4f})\")\n",
    "\n",
    "    result_file = os.path.join(result_dir, f'Results_{model_choice}.txt')\n",
    "\n",
    "    # Save results to text file\n",
    "    with open(result_file, 'w') as f:\n",
    "        f.write(f\"Model: {model_choice}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy_mean:.4f} (95% CI: {accuracy_ci[0]:.4f}, {accuracy_ci[1]:.4f})\\n\")\n",
    "        f.write(f\"AUC: {auc_mean:.4f} (95% CI: {auc_ci[0]:.4f}, {auc_ci[1]:.4f})\\n\")\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df_sorted.head(20))\n",
    "    plt.title('Top 20 Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(result_dir, 'feature_importance.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Retrain model on the entire training data for final evaluation\n",
    "    # Use the top 20 features based on importance\n",
    "    top_features = importance_df_sorted['Feature'].tolist()[:20]\n",
    "    X_train_selected = X_train[top_features]\n",
    "    X_val_selected = X_val[top_features]\n",
    "    model = clone(model_init)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    y_pred = model.predict(X_val_selected)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        conf_matrix,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Control', 'Cancer'],  # Adjust labels as needed\n",
    "        yticklabels=['Control', 'Cancer'],\n",
    "        annot_kws={\"size\": 14}\n",
    "    )\n",
    "    plt.title('Confusion Matrix on Validation Set', fontsize=15)\n",
    "    plt.xlabel('Predicted Label', fontsize=13)\n",
    "    plt.ylabel('True Label', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(result_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # ROC curve and AUC score for the final model\n",
    "    y_pred_proba = model.predict_proba(X_val_selected)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "    auc_score = roc_auc_score(y_val, y_pred_proba)\n",
    "\n",
    "    # ROC curve plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {auc_score:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16)\n",
    "    plt.legend(loc='lower right', fontsize=12)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(result_dir, 'roc_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # SHAP explanation (on the selected features)\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_val_selected)\n",
    "\n",
    "    # SHAP summary plot\n",
    "    shap.summary_plot(shap_values, X_val_selected, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(result_dir, 'shap_feature_importance.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"All results are saved in {result_dir}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Number of bootstrap samples widget\n",
    "bootstrap_selector = widgets.IntText(\n",
    "    value=100,\n",
    "    description='Bootstraps:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Model selection widget\n",
    "model_selector = widgets.Dropdown(\n",
    "    options=['xgb', 'rf', 'catboost', 'gbm', 'lgbm'],\n",
    "    value='xgb',\n",
    "    description='Choose Model:',\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(description=\"Run Model\")\n",
    "\n",
    "# Label to indicate model status\n",
    "status_label = widgets.Label(value='')\n",
    "\n",
    "\n",
    "def on_button_click(b):\n",
    "    # Update status to \"Running...\"\n",
    "    status_label.value = \"Running...\"\n",
    "    result = run_model(model_selector.value, n_bootstraps=bootstrap_selector.value)\n",
    "    # After running, update status to \"Done!\"\n",
    "    status_label.value = \"Done!\"\n",
    "    \n",
    "run_button.on_click(on_button_click)\n",
    "\n",
    "# Display widgets\n",
    "display(model_selector)\n",
    "display(bootstrap_selector)\n",
    "display(run_button)\n",
    "display(status_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparing discovery set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167137228fee487c83f344ffd1480c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Choose Model:', options=('xgb', 'rf', 'catboost', 'gbm', 'lgbm'), value='xgb')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c50e5973d05416b8f3dcc3d40835502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Model', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd44bbb78b24a44a45ebe134dd5ffcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1/5\n",
      "Saved combined train/validation set for fold 1 at Train_Result_RFE/xgb_20241107-143049/combined_train_val_fold_1.xlsx\n",
      "Saved feature selection counts at Train_Result_RFE/xgb_20241107-143049/feature_selection_counts.xlsx\n",
      "Saved confusion matrix for fold 1 at Train_Result_RFE/xgb_20241107-143049/confusion_matrix_fold_1.png\n",
      "Permutation importance saved to Train_Result_RFE/xgb_20241107-143049/permutation_importance_train_fold_1_xgb.csv\n",
      "Permutation importance saved to Train_Result_RFE/xgb_20241107-143049/permutation_importance_val_fold_1_xgb.csv\n",
      "Saved feature importance plot for train at Train_Result_RFE/xgb_20241107-143049/feature_importance_train_fold_{0}.png\n",
      "Saved feature importance plot for valid at Train_Result_RFE/xgb_20241107-143049/feature_importance_valid_fold_{0}.png\n",
      "Saved training important features for fold 1 at Train_Result_RFE/xgb_20241107-143049/train_important_features_fold_1.txt\n",
      "Saved validation important features for fold 1 at Train_Result_RFE/xgb_20241107-143049/val_important_features_fold_1.txt\n",
      "Saved common features for fold 1 at Train_Result_RFE/xgb_20241107-143049/common_features_fold_1.txt\n",
      "Running fold 2/5\n",
      "Saved combined train/validation set for fold 2 at Train_Result_RFE/xgb_20241107-143049/combined_train_val_fold_2.xlsx\n",
      "Saved feature selection counts at Train_Result_RFE/xgb_20241107-143049/feature_selection_counts.xlsx\n",
      "Saved confusion matrix for fold 2 at Train_Result_RFE/xgb_20241107-143049/confusion_matrix_fold_2.png\n",
      "Permutation importance saved to Train_Result_RFE/xgb_20241107-143049/permutation_importance_train_fold_2_xgb.csv\n",
      "Permutation importance saved to Train_Result_RFE/xgb_20241107-143049/permutation_importance_val_fold_2_xgb.csv\n",
      "Saved feature importance plot for train at Train_Result_RFE/xgb_20241107-143049/feature_importance_train_fold_{1}.png\n",
      "Saved feature importance plot for valid at Train_Result_RFE/xgb_20241107-143049/feature_importance_valid_fold_{1}.png\n",
      "Saved training important features for fold 2 at Train_Result_RFE/xgb_20241107-143049/train_important_features_fold_2.txt\n",
      "Saved validation important features for fold 2 at Train_Result_RFE/xgb_20241107-143049/val_important_features_fold_2.txt\n",
      "Saved common features for fold 2 at Train_Result_RFE/xgb_20241107-143049/common_features_fold_2.txt\n",
      "Running fold 3/5\n",
      "Saved combined train/validation set for fold 3 at Train_Result_RFE/xgb_20241107-143049/combined_train_val_fold_3.xlsx\n",
      "Saved feature selection counts at Train_Result_RFE/xgb_20241107-143049/feature_selection_counts.xlsx\n",
      "Saved confusion matrix for fold 3 at Train_Result_RFE/xgb_20241107-143049/confusion_matrix_fold_3.png\n",
      "Permutation importance saved to Train_Result_RFE/xgb_20241107-143049/permutation_importance_train_fold_3_xgb.csv\n",
      "Permutation importance saved to Train_Result_RFE/xgb_20241107-143049/permutation_importance_val_fold_3_xgb.csv\n",
      "Saved feature importance plot for train at Train_Result_RFE/xgb_20241107-143049/feature_importance_train_fold_{2}.png\n",
      "Saved feature importance plot for valid at Train_Result_RFE/xgb_20241107-143049/feature_importance_valid_fold_{2}.png\n",
      "Saved training important features for fold 3 at Train_Result_RFE/xgb_20241107-143049/train_important_features_fold_3.txt\n",
      "Saved validation important features for fold 3 at Train_Result_RFE/xgb_20241107-143049/val_important_features_fold_3.txt\n",
      "Saved common features for fold 3 at Train_Result_RFE/xgb_20241107-143049/common_features_fold_3.txt\n",
      "Running fold 4/5\n",
      "Saved combined train/validation set for fold 4 at Train_Result_RFE/xgb_20241107-143049/combined_train_val_fold_4.xlsx\n",
      "Saved feature selection counts at Train_Result_RFE/xgb_20241107-143049/feature_selection_counts.xlsx\n",
      "Saved confusion matrix for fold 4 at Train_Result_RFE/xgb_20241107-143049/confusion_matrix_fold_4.png\n",
      "Permutation importance saved to Train_Result_RFE/xgb_20241107-143049/permutation_importance_train_fold_4_xgb.csv\n",
      "Permutation importance saved to Train_Result_RFE/xgb_20241107-143049/permutation_importance_val_fold_4_xgb.csv\n",
      "Saved feature importance plot for train at Train_Result_RFE/xgb_20241107-143049/feature_importance_train_fold_{3}.png\n",
      "Saved feature importance plot for valid at Train_Result_RFE/xgb_20241107-143049/feature_importance_valid_fold_{3}.png\n",
      "Saved training important features for fold 4 at Train_Result_RFE/xgb_20241107-143049/train_important_features_fold_4.txt\n",
      "Saved validation important features for fold 4 at Train_Result_RFE/xgb_20241107-143049/val_important_features_fold_4.txt\n",
      "Saved common features for fold 4 at Train_Result_RFE/xgb_20241107-143049/common_features_fold_4.txt\n",
      "Running fold 5/5\n",
      "Saved combined train/validation set for fold 5 at Train_Result_RFE/xgb_20241107-143049/combined_train_val_fold_5.xlsx\n",
      "Saved feature selection counts at Train_Result_RFE/xgb_20241107-143049/feature_selection_counts.xlsx\n",
      "Saved confusion matrix for fold 5 at Train_Result_RFE/xgb_20241107-143049/confusion_matrix_fold_5.png\n",
      "Permutation importance saved to Train_Result_RFE/xgb_20241107-143049/permutation_importance_train_fold_5_xgb.csv\n",
      "Permutation importance saved to Train_Result_RFE/xgb_20241107-143049/permutation_importance_val_fold_5_xgb.csv\n",
      "Saved feature importance plot for train at Train_Result_RFE/xgb_20241107-143049/feature_importance_train_fold_{4}.png\n",
      "Saved feature importance plot for valid at Train_Result_RFE/xgb_20241107-143049/feature_importance_valid_fold_{4}.png\n",
      "Saved training important features for fold 5 at Train_Result_RFE/xgb_20241107-143049/train_important_features_fold_5.txt\n",
      "Saved validation important features for fold 5 at Train_Result_RFE/xgb_20241107-143049/val_important_features_fold_5.txt\n",
      "Saved common features for fold 5 at Train_Result_RFE/xgb_20241107-143049/common_features_fold_5.txt\n",
      "Saved training feature importance counts at Train_Result_RFE/xgb_20241107-143049/train_feature_importance_counts.xlsx\n",
      "Saved validation feature importance counts at Train_Result_RFE/xgb_20241107-143049/val_feature_importance_counts.xlsx\n",
      "Saved common feature counts across all folds at Train_Result_RFE/xgb_20241107-143049/common_feature_counts.xlsx\n",
      "\n",
      "Cross-validation results for xgb:\n",
      "Mean Accuracy: 0.8886\n",
      "Mean FNR: 0.4234\n",
      "All cross-validation results saved in Train_Result_RFE/xgb_20241107-143049\n",
      "Saved ROC curves plot at Train_Result_RFE/xgb_20241107-143049/cv_roc_curves.png\n",
      "Model training and evaluation completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import shap\n",
    "import os\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ipywidgets as widgets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from IPython.display import display\n",
    "from sklearn.feature_selection import RFE\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from collections import Counter\n",
    "import os\n",
    "path = os.getcwd()\n",
    "data = pd.read_excel(path+\"/PC_normalized_PC92_HC384_130 genus_ML.xlsx\")\n",
    "data.set_index('study_no',inplace=True)\n",
    "label_column = 'group_1'\n",
    "\n",
    "# Define feature columns (adjust the column indices as needed)\n",
    "feature_columns = data.columns[3:]  # Assuming features start from the 4th column\n",
    "\n",
    "# Extract features and labels\n",
    "features = data[feature_columns]\n",
    "label = data[label_column]\n",
    "\n",
    "\n",
    "# Convert features and label to appropriate data types\n",
    "features = features.astype(float)\n",
    "label = label.astype(int)  # Or float, depending on your label encoding\n",
    "\n",
    "\n",
    "# FNR evaluation function\n",
    "def FNR_eval(y_pred_proba, y_true):\n",
    "    y_pred_label = (y_pred_proba >= 0.5).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_label).ravel()\n",
    "    return fn / (fn + tp)  # FN / (FN + TP)\n",
    "\n",
    "# Argument parser\n",
    "parser = argparse.ArgumentParser(description='Train and evaluate a binary classification model.')\n",
    "parser.add_argument('--model', type=str, default='xgb', choices=['xgb', 'rf', 'catboost', 'gbm', 'lgbm'],\n",
    "                    help='Choose a model: xgb (XGBoost), rf (Random Forest), catboost, gbm, lgbm (LightGBM).')\n",
    "parser.add_argument('--random_state', type=int, default=42, help='Random state for reproducibility.')\n",
    "parser.add_argument('--n_estimators', type=int, default=100, help='Number of trees/estimators for the model.')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.1, help='Learning rate for boosting models.')\n",
    "parser.add_argument('--max_depth', type=int, default=6, help='Maximum depth for tree-based models.')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# Initialize model\n",
    "def initialize_model(model_choice,args):\n",
    "    if model_choice == 'xgb':\n",
    "        return xgb.XGBClassifier(n_estimators=args.n_estimators, learning_rate=args.learning_rate, \n",
    "                                 max_depth=args.max_depth, random_state=args.random_state,tree_method ='hist', device = 'cuda')\n",
    "    elif model_choice == 'rf':\n",
    "        return RandomForestClassifier(n_estimators=args.n_estimators, max_depth=args.max_depth, \n",
    "                                      random_state=args.random_state,n_jobs=-1)\n",
    "    elif model_choice == 'catboost':\n",
    "        return CatBoostClassifier(iterations=args.n_estimators, depth=args.max_depth, \n",
    "                                  learning_rate=args.learning_rate, random_state=args.random_state, \n",
    "                                  verbose=0,task_type='GPU')\n",
    "    elif model_choice == 'gbm':\n",
    "        return GradientBoostingClassifier(n_estimators=args.n_estimators, learning_rate=args.learning_rate, \n",
    "                                          max_depth=args.max_depth, random_state=args.random_state)\n",
    "    elif model_choice == 'lgbm':\n",
    "        return lgb.LGBMClassifier(n_estimators=args.n_estimators, learning_rate=args.learning_rate, \n",
    "                                  max_depth=args.max_depth, random_state=args.random_state, device='gpu')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_choice}\")\n",
    "    \n",
    "\n",
    "def create_output_dir(model_choice):\n",
    "    # Use model's short name, not the full representation\n",
    "    timestamp = time.strftime('%Y%m%d-%H%M%S')  # Short timestamp\n",
    "    dir_name = f\"{model_choice}_{timestamp}\"  # Simple name based on model type and timestamp\n",
    "    run_output_dir = os.path.join('Train_Result_RFE', dir_name)\n",
    "    os.makedirs(run_output_dir, exist_ok=True)\n",
    "    return run_output_dir\n",
    "\n",
    "# RFE Feature Selection\n",
    "def rfe_feature_selection(model, X, y, n_features):\n",
    "    rfe = RFE(model, n_features_to_select=n_features)\n",
    "    rfe.fit(X, y)\n",
    "    selected_features_mask = rfe.support_  # Boolean mask of selected features\n",
    "    return X.columns[selected_features_mask], rfe\n",
    "\n",
    "def refit_model(model, X_selected, y):\n",
    "    model.fit(X_selected, y)  # Just fit the passed-in model\n",
    "    return model  # Return the refitted model\n",
    "\n",
    "# Model evaluation (accuracy, FNR, confusion matrix)\n",
    "def evaluate_model(model_refit, X_val_selected, y_val, model_choice, run_output_dir, fold_number):\n",
    "    val_pred = model_refit.predict(X_val_selected)\n",
    "    y_pred_proba = model_refit.predict_proba(X_val_selected)[:, 1]\n",
    "    accuracy = accuracy_score(y_val, val_pred)\n",
    "    fnr = FNR_eval(y_pred_proba, y_val)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_val, val_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Control', 'Cancer'], \n",
    "                yticklabels=['Control', 'Cancer'], annot_kws={\"size\": 14})\n",
    "    plt.title('Confusion Matrix on Validation Set', fontsize=15)\n",
    "    plt.xlabel('Predicted Label', fontsize=13)\n",
    "    plt.ylabel('True Label', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_file = os.path.join(run_output_dir, f'confusion_matrix_fold_{fold_number}.png')\n",
    "    plt.savefig(plot_file)\n",
    "    plt.close()\n",
    "    print(f\"Saved confusion matrix for fold {fold_number} at {plot_file}\")\n",
    "\n",
    "    return accuracy, fnr\n",
    "\n",
    "# Permutation importance\n",
    "def compute_permutation_importance(model_refit, X_selected, y, random_state):\n",
    "    perm_importance = PermutationImportance(model_refit, random_state=random_state).fit(X_selected, y)\n",
    "    return perm_importance.feature_importances_\n",
    "\n",
    "# Function to save permutation importance\n",
    "def save_permutation_importance(importances, features, run_output_dir, model_choice, mode):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Save to CSV or Excel (adjust according to your preference)\n",
    "    importance_file = os.path.join(run_output_dir, f'permutation_importance_{mode}_{model_choice}.csv')\n",
    "    importance_df.to_csv(importance_file, index=False)\n",
    "    print(f\"Permutation importance saved to {importance_file}\")\n",
    "          \n",
    "# SHAP values and plots\n",
    "def plot_shap_values(model_refit, X_selected, run_output_dir, fold_number, dataset_type, model_choice):\n",
    "    explainer = shap.TreeExplainer(model_refit)\n",
    "    shap_values = explainer.shap_values(X_selected)\n",
    "    \n",
    "    if model_choice == 'rf':\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values[:, :, 1], X_selected, plot_type='dot', show=False)\n",
    "    else:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, X_selected, show=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(run_output_dir, f'shap_feature_importance_{fold_number}.png'))\n",
    "    plt.close()\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# Plot ROC curve across cross-validation folds\n",
    "def plot_cv_roc_curves(model, X, y, skf, model_choice, run_output_dir):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Calculate ROC and AUC for this fold\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Interpolate TPR for mean ROC curve\n",
    "        tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        aucs.append(roc_auc)\n",
    "\n",
    "        # Plot each fold's ROC curve\n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.3, label=f'Fold {i+1} AUC: {roc_auc:.4f}')\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, color='orange', lw=2, alpha=0.8,\n",
    "             label=r'Mean ROC (AUC = %0.4f ± %0.4f)' % (mean_auc, std_auc))\n",
    "    # Plot standard deviation (shading)\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tpr_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tpr_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(mean_fpr, tpr_lower, tpr_upper, color='grey', alpha=0.2,\n",
    "                     label=r'± 1 std. dev.')\n",
    "\n",
    "    # Plot settings\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Cross-Validation ROC Curves - {model_choice}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    plot_file = os.path.join(run_output_dir, 'cv_roc_curves.png')\n",
    "    plt.savefig(plot_file)\n",
    "    plt.close()\n",
    "    print(f\"Saved ROC curves plot at {plot_file}\")\n",
    "\n",
    "\n",
    "def plot_feature_importance(importances, feature_names, dataset_type, fold_number, run_output_dir):\n",
    "    # Create a DataFrame for plotting\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    cmap = sns.color_palette(\"rainbow\", as_cmap = True)\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, len(importance_df))]\n",
    "    \n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df, palette=colors, hue='Feature', dodge=False, legend=False)\n",
    "    plt.title(f'Feature Importances ({dataset_type.capitalize()} - Fold {fold_number})')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plot_file = os.path.join(run_output_dir, f'feature_importance_{dataset_type}_fold_{fold_number}.png')\n",
    "    plt.savefig(plot_file)\n",
    "    plt.close()\n",
    "    print(f\"Saved feature importance plot for {dataset_type} at {plot_file}\")\n",
    "\n",
    "\n",
    "\n",
    "def run_model(model_choice,n_splits = 5):\n",
    "    \"\"\"\n",
    "    Runs the machine learning model with cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - model_choice (str): The chosen model type.\n",
    "    - features (DataFrame): The feature set.\n",
    "    - label (Series): The target variable.\n",
    "    - n_splits (int): Number of cross-validation splits.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "        \n",
    "    # Output directory using model choice, not full model object\n",
    "    run_output_dir = create_output_dir(model_choice)\n",
    "\n",
    "    # Stratified K-Fold to preserve the class balance\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=args.random_state)\n",
    "    \n",
    "\n",
    "    accuracy_list = []\n",
    "    fnr_list = []\n",
    "    train_importances_list = []\n",
    "    val_importances_list = []\n",
    "    common_features_list = []\n",
    "\n",
    "    # Initialize a Counter to keep track of feature selections\n",
    "    feature_counter = Counter()\n",
    "    # Initialize Counters to keep track of important features\n",
    "    train_feature_importance_counter = Counter()\n",
    "    val_feature_importance_counter = Counter()\n",
    "   \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(features, label)):\n",
    "        print(f\"Running fold {fold+1}/{n_splits}\")\n",
    "\n",
    "        # Data for current fold\n",
    "        X_train, X_val = features.iloc[train_idx], features.iloc[val_idx]\n",
    "        y_train, y_val = label.iloc[train_idx], label.iloc[val_idx]\n",
    "\n",
    "        # Merge X_train and y_train, and X_val and y_val, and save them for inspection\n",
    "        train_data = X_train.copy()\n",
    "        train_data['label'] = y_train\n",
    "        train_data['set'] = 'Discovery'\n",
    "\n",
    "        val_data = X_val.copy()\n",
    "        val_data['label'] = y_val\n",
    "        val_data['set'] = 'Validation'\n",
    "\n",
    "        # Combine train and validation data\n",
    "        combined_data = pd.concat([train_data, val_data])\n",
    "\n",
    "        # Save combined data to an Excel file for this fold\n",
    "        combined_file_path = os.path.join(run_output_dir, f'combined_train_val_fold_{fold+1}.xlsx')\n",
    "        combined_data.to_excel(combined_file_path, index=True)\n",
    "\n",
    "        print(f\"Saved combined train/validation set for fold {fold+1} at {combined_file_path}\")\n",
    "        \n",
    "        # Step 1: Initialize model for training\n",
    "        model = initialize_model(model_choice,args=args)\n",
    "\n",
    "        # Step 2: RFE for training set\n",
    "        selected_features, rfe_all = rfe_feature_selection(model, X_train, y_train, n_features=20)\n",
    "        X_train_selected = X_train[selected_features]\n",
    "\n",
    "\n",
    "        # Update feature_counter with the selected features from this fold\n",
    "        feature_counter.update(selected_features)\n",
    "\n",
    "\n",
    "        # Convert the counter to a DataFrame\n",
    "        feature_counts_df = pd.DataFrame.from_dict(feature_counter, orient='index', columns=['count'])\n",
    "        feature_counts_df = feature_counts_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "        # Save to Excel or CSV\n",
    "        feature_counts_file = os.path.join(run_output_dir, 'feature_selection_counts.xlsx')\n",
    "        feature_counts_df.to_excel(feature_counts_file)\n",
    "        print(f\"Saved feature selection counts at {feature_counts_file}\")\n",
    "\n",
    "        # Train model on training set with selected features\n",
    "        model_refit_train = refit_model(model, X_train_selected, y_train)\n",
    "\n",
    "        # Apply the same selected features to validation set\n",
    "        X_val_selected = X_val[selected_features]\n",
    "\n",
    "        # Step 4: Evaluate model on validation set\n",
    "        accuracy, fnr = evaluate_model(model_refit_train, X_val_selected, y_val, model_choice, run_output_dir, fold_number=fold+1)\n",
    "\n",
    "        # Append accuracy and FNR for this fold\n",
    "        accuracy_list.append(accuracy)\n",
    "        fnr_list.append(fnr)\n",
    "\n",
    "        # Step 5: Permutation importance for both datasets using the trained model\n",
    "        train_importances = compute_permutation_importance(model_refit_train, X_train_selected, y_train, args.random_state)\n",
    "        val_importances = compute_permutation_importance(model_refit_train, X_val_selected, y_val, args.random_state)\n",
    "\n",
    "        # Store feature importances for comparison\n",
    "        train_importances_list.append(train_importances)\n",
    "        val_importances_list.append(val_importances)\n",
    "\n",
    "        # Save permutation importance results for both training and validation sets\n",
    "        save_permutation_importance(train_importances, X_train_selected.columns, run_output_dir, model_choice, f\"train_fold_{fold+1}\")\n",
    "        save_permutation_importance(val_importances, X_val_selected.columns, run_output_dir, model_choice, f\"val_fold_{fold+1}\")\n",
    "\n",
    "        plot_feature_importance(train_importances,X_train_selected.columns, dataset_type='train',fold_number ={fold},run_output_dir=run_output_dir)\n",
    "        plot_feature_importance(val_importances, X_val_selected.columns, dataset_type='valid',fold_number = {fold},run_output_dir=run_output_dir)\n",
    "        \n",
    "        # After computing permutation importances\n",
    "        # For training data\n",
    "        # For training data\n",
    "        plot_shap_values(model_refit_train, X_train_selected, run_output_dir, fold+1, 'train', model_choice)\n",
    "\n",
    "        # For validation data\n",
    "        plot_shap_values(model_refit_train, X_val_selected, run_output_dir, fold+1, 'validation', model_choice)\n",
    "\n",
    "\n",
    "        # Step 6: Find common features between training and validation sets\n",
    "        train_important_features = X_train_selected.columns[train_importances > 0]\n",
    "        train_feature_importance_counter.update(train_important_features)\n",
    "\n",
    "        # For validation data\n",
    "        val_important_features = X_val_selected.columns[val_importances > 0]\n",
    "        val_feature_importance_counter.update(val_important_features)\n",
    "\n",
    "        # [Optional] Save important features for this fold\n",
    "        # Save training important features\n",
    "        train_features_file = os.path.join(run_output_dir, f'train_important_features_fold_{fold+1}.txt')\n",
    "        with open(train_features_file, 'w') as f:\n",
    "            for feature in train_important_features:\n",
    "                f.write(f\"{feature}\\n\")\n",
    "        print(f\"Saved training important features for fold {fold+1} at {train_features_file}\")\n",
    "\n",
    "        # Save validation important features\n",
    "        val_features_file = os.path.join(run_output_dir, f'val_important_features_fold_{fold+1}.txt')\n",
    "        with open(val_features_file, 'w') as f:\n",
    "            for feature in val_important_features:\n",
    "                f.write(f\"{feature}\\n\")\n",
    "        print(f\"Saved validation important features for fold {fold+1} at {val_features_file}\")\n",
    "\n",
    "        # Intersection of selected features in train and validation\n",
    "        common_features = train_important_features.intersection(val_important_features)\n",
    "        common_features_list.append(common_features)\n",
    "\n",
    "        # Save the common features for this fold\n",
    "        common_features_file = os.path.join(run_output_dir, f'common_features_fold_{fold+1}.txt')\n",
    "        with open(common_features_file, 'w') as f:\n",
    "            for feature in common_features:\n",
    "                f.write(f\"{feature}\\n\")\n",
    "        print(f\"Saved common features for fold {fold+1} at {common_features_file}\")\n",
    "\n",
    "        \n",
    "\n",
    "    # After the CV loop, process the feature importance counters\n",
    "\n",
    "    # Convert the counters to DataFrames\n",
    "    train_feature_counts_df = pd.DataFrame.from_dict(train_feature_importance_counter, orient='index', columns=['count'])\n",
    "    train_feature_counts_df = train_feature_counts_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "    val_feature_counts_df = pd.DataFrame.from_dict(val_feature_importance_counter, orient='index', columns=['count'])\n",
    "    val_feature_counts_df = val_feature_counts_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "    # Save the feature counts to Excel files\n",
    "    train_feature_counts_file = os.path.join(run_output_dir, 'train_feature_importance_counts.xlsx')\n",
    "    train_feature_counts_df.to_excel(train_feature_counts_file)\n",
    "    print(f\"Saved training feature importance counts at {train_feature_counts_file}\")\n",
    "\n",
    "    val_feature_counts_file = os.path.join(run_output_dir, 'val_feature_importance_counts.xlsx')\n",
    "    val_feature_counts_df.to_excel(val_feature_counts_file)\n",
    "    print(f\"Saved validation feature importance counts at {val_feature_counts_file}\")\n",
    "\n",
    "    # [Optional] Save common features across all folds\n",
    "    # Flatten the list of common features and count them\n",
    "    all_common_features = [feature for fold_features in common_features_list for feature in fold_features]\n",
    "    common_feature_counter = Counter(all_common_features)\n",
    "    common_features_df = pd.DataFrame.from_dict(common_feature_counter, orient='index', columns=['count'])\n",
    "    common_features_df = common_features_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "    common_features_file = os.path.join(run_output_dir, 'common_feature_counts.xlsx')\n",
    "    common_features_df.to_excel(common_features_file)\n",
    "    print(f\"Saved common feature counts across all folds at {common_features_file}\")\n",
    "\n",
    "    # After all folds, compute mean performance metrics\n",
    "    mean_accuracy = np.mean(accuracy_list)\n",
    "    mean_fnr = np.mean(fnr_list)\n",
    "\n",
    "    # Print cross-validation results\n",
    "    print(f\"\\nCross-validation results for {model_choice}:\")\n",
    "    print(f\"Mean Accuracy: {mean_accuracy:.4f}\")\n",
    "    print(f\"Mean FNR: {mean_fnr:.4f}\")\n",
    "\n",
    "    result_file = os.path.join(run_output_dir, f'CV_Results_{model_choice}.txt')\n",
    "    with open(result_file, 'w') as f:\n",
    "        f.write(f\"Model: {model_choice}\\n\")\n",
    "        f.write(f\"Mean Accuracy: {mean_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Mean FNR: {mean_fnr:.4f}\\n\")\n",
    "    print(f\"All cross-validation results saved in {run_output_dir}\")\n",
    "\n",
    "\n",
    "    # Plot ROC curves\n",
    "    plot_cv_roc_curves(model, features, label, skf, model_choice, run_output_dir)\n",
    "    print(\"Model training and evaluation completed.\")\n",
    "\n",
    "# Widgets for model selection\n",
    "model_selector = widgets.Dropdown(\n",
    "    options=['xgb', 'rf', 'catboost', 'gbm', 'lgbm'],\n",
    "    value='xgb',\n",
    "    description='Choose Model:',\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(description=\"Run Model\")\n",
    "status_label = widgets.Label(value='')\n",
    "\n",
    "def on_button_click(b):\n",
    "    status_label.value = \"Running...\"\n",
    "    run_model(model_selector.value,n_splits=5)  # Pass model type (e.g., 'xgb') instead of initializing the model\n",
    "    status_label.value = \"Done!\"\n",
    "\n",
    "run_button.on_click(on_button_click)\n",
    "\n",
    "# Display widgets\n",
    "display(model_selector)\n",
    "display(run_button)\n",
    "display(status_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PcML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
